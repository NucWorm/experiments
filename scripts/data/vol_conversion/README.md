# H5 to TIFF Conversion

This directory contains scripts to convert neuropal H5 volumes to TIFF stacks for use in the NucWorm benchmark.

## ğŸ“ **Directory Structure**

```
vol_conversion/
â”œâ”€â”€ README.md              # This file
â”œâ”€â”€ environment.yml        # Conda environment specification
â”œâ”€â”€ src/                  # Core conversion scripts
â”‚   â”œâ”€â”€ convert_h5_to_tiff.py
â”‚   â””â”€â”€ check_tiff_shape.py
â”œâ”€â”€ scripts/              # Slurm execution scripts
â”‚   â””â”€â”€ run_h5_to_tiff_conversion.slurm
â””â”€â”€ docs/                 # Documentation
    â”œâ”€â”€ PREFERRED_SLURM_PARAMS.md
    â””â”€â”€ [legacy README.md]
```

## ğŸš€ **Quick Start**

### **Run Full Conversion Pipeline**
```bash
cd /projects/weilab/gohaina/nucworm/scripts/data/vol_conversion
sbatch scripts/run_h5_to_tiff_conversion.slurm
```

This will:
- Create `h5_conversion` conda environment
- Run 3 parallel array jobs (one per dataset)
- Convert all H5 files to TIFF format
- Save to `/projects/weilab/gohaina/nucworm/outputs/data/neuropal_as_tiff/`

## ğŸ“Š **Datasets Processed**

- **nejatbakhsh20**: 21 volumes
- **yemini21**: 10 volumes  
- **wen20**: 9 volumes

## ğŸ”§ **Technical Details**

### **Data Format Conversion**
- **Input**: H5 files with shape `(Z, C, Y, X)`
- **Output**: Multi-page TIFF files in `(C, Z, Y, X)` format
- **Format**: uint8, no normalization needed
- **Compatibility**: Optimized for nnUNet processing

### **Resource Requirements**
- **Partition**: `weilab`
- **Memory**: 32GB per array task
- **CPUs**: 2 per array task
- **Time**: 2 hours per dataset

## ğŸ“ˆ **Output Structure**

```
/projects/weilab/gohaina/nucworm/outputs/data/neuropal_as_tiff/
â”œâ”€â”€ nejatbakhsh20/
â”‚   â”œâ”€â”€ 000541/                    â† Case groups
â”‚   â””â”€â”€ ...
â”œâ”€â”€ yemini21/
â”‚   â”œâ”€â”€ 000715/                    â† Case groups  
â”‚   â””â”€â”€ ...
â””â”€â”€ wen20/
    â”œâ”€â”€ 000692/                    â† Case groups
    â””â”€â”€ ...
```

## ğŸ” **Monitoring**

### **Check Job Status**
```bash
squeue -u gohaina
```

### **View Logs**
```bash
# Monitor specific array tasks:
tail -f /projects/weilab/gohaina/logs/h5_to_tiff_conversion_<jobid>_0.out  # nejatbakhsh20
tail -f /projects/weilab/gohaina/logs/h5_to_tiff_conversion_<jobid>_1.out  # yemini21  
tail -f /projects/weilab/gohaina/logs/h5_to_tiff_conversion_<jobid>_2.out  # wen20
```

## ğŸš¨ **Troubleshooting**

### **Common Issues**
1. **Environment creation fails**: Check conda availability and environment.yml validity
2. **Memory issues**: Increase memory allocation in slurm script
3. **Time limits**: Increase time allocation if conversion is slow

### **Environment Management**
- **Remove environment**: `conda env remove -n h5_conversion`
- **Recreate environment**: Scripts automatically recreate if needed
- **Check environment**: `conda env list`

## ğŸ”— **Integration**

This conversion step is automatically called by:
- `methods/nnunet/scripts/run_full_pipeline.slurm`

The converted TIFF files are then used by all benchmark methods for nuclei detection.

## ğŸ“ **Notes**

- This is a shared data processing utility used by all benchmark methods
- Output files are stored in the shared outputs directory
- Environment is automatically managed by the scripts
- Array jobs provide efficient parallel processing


